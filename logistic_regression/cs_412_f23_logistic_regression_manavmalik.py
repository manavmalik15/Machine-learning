# -*- coding: utf-8 -*-
"""CS 412-F23-logistic-regression_ManavMalik.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CqeTpq53VHz6IIi9zoxzlhtjilFMSavX

Name: Manav Malik
"""



#Mounting Google Drive:
#After running this cell a popup window will appear and requesting to select your  Google account and give the access permission.
#You can either use your personal Google account or your UIC Google account.
from google.colab import drive
drive.mount('/content/gdrive')

#You need to change this path to get the data
path="/content/gdrive/MyDrive/CS412project2/"

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
style.use('ggplot')
# %matplotlib inline
import re
import pandas as pd

"""Numpy is library for scientific computing in Python. It has efficient implementation of n-dimensional array (tensor) manupulations, which is useful for machine learning applications."""

import numpy as np

"""We can convert a list into numpy array (tensor)  """

b = [[1, 2, 4], [2, 6, 9]]
a = np.array(b)
a

"""We can check the dimensions of the array"""

a.shape

"""We can apply simple arithmetic operation on all element of a tensor"""

a * 3

"""You can transpose a tensor
    
"""

print(a.T.shape)
a.T

"""You can apply aggregate functions on the whole tensor"""

np.sum(a)

"""or on one dimension of it"""

np.sum(a, axis=0)

np.sum(a, axis=1)



"""We can do element-wise arithmetic operation on two tensors (of the same size)"""

c1 = np.array([[1, 2, 4], [2, 6, 9]])
c2 = np.array([[2, 3, 5], [1, 2, 1]])
c1 * c2

"""If you want to multiply all columns of a tensor by vector (for example if you want to multiply all data features by their lables) you need a trick. This multiplication shows up in calculating the gradients."""

a = np.array([[1, 2, 4], [2, 6, 9]])
b = np.array([1,-1])
print(a)
print(b)

"""Here we want to multiply the first row of a by 1 and the second row of a by -1. Simply multiplying a by b does not work because a and b do not have the same dimension"""

#a * b

"""To do this multiplication we first have to assume b has one column and then repeat the column of b with the number of columns in a. We use tile function to do that"""

b_repeat = np.tile(b,  (a.shape[1],1)).T
print(b_repeat.shape)
b_repeat

"""Now we can multiply each column of a by b:"""

a * b_repeat

"""You can create inital random vector using numpy (using N(0,1)):"""

mu = 0 #mean
sigma = 1 #standard deviation
r = np.random.normal(mu,sigma, 1000) #draws 1000 samples from a normal distribution

"""We can apply functions on tensors"""

#implementation of Normal distribution
def normal(x, mu, sigma):
    return np.exp( -0.5 * ((x-mu)/sigma)**2)/np.sqrt(2.0*np.pi*sigma**2)

#probability of samples on the Normal distribution
probabilities = normal(r, mu, sigma)

"""Numpy has useful APIs for analysis. Here we plot the histogram of samples and also plot the probabilies to see if the samples follow the normal distribution."""

counts, bins = np.histogram(r,50,density=True)
plt.hist(bins[:-1], bins, weights=counts)
plt.scatter(r, probabilities, c='b', marker='.')

def read_data(filename):
    f = open(filename, 'r')
    p = re.compile(',')
    xdata = []
    ydata = []
    header = f.readline().strip()
    varnames = p.split(header)
    namehash = {}
    for l in f:
        li = p.split(l.strip())
        xdata.append([float(x) for x in li[:-1]])
        ydata.append(float(li[-1]))

    return np.array(xdata), np.array(ydata)

"""**Assuming** our data is x is available in numpy we use numpy to implement logistic regression

"""

(xtrain_whole, ytrain_whole) = read_data(path + 'spambase-train.csv')
(xtest, ytest) = read_data(path + 'spambase-test.csv')

print("The shape of xtrain:", xtrain_whole.shape)
print("The shape of ytrain:", ytrain_whole.shape)
print("The shape of xtest:", xtest.shape)
print("The shape of ytest:", ytest.shape)

"""before training we normalize the input data (features)"""

xmean = np.mean(xtrain_whole, axis=0)
xstd = np.std(xtrain_whole, axis=0)
xtrain_normal_whole = (xtrain_whole-xmean) / xstd
xtest_normal = (xtest-xmean) / xstd

"""We need to create a validation set. We create an array of indecies and permute it."""

premute_indicies = np.random.permutation(np.arange(xtrain_whole.shape[0]))

"""We keep the first 2600 data points as the training data and rest as the validation data"""

xtrain_normal = xtrain_normal_whole[premute_indicies[:2600]]
ytrain = ytrain_whole[premute_indicies[:2600]]
xval_normal = xtrain_normal_whole[premute_indicies[2600:]]
yval = ytrain_whole[premute_indicies[2600:]]

"""Initiallizing the weights and bias with random values from N(0,1)"""

weights = np.random.normal(0, 1, xtrain_normal.shape[1]);
bias = np.random.normal(0,1,1)

#the sigmoid function
def sigmoid(v):
    #return np.exp(-np.logaddexp(0, -v)) #numerically stable implementation of sigmoid function
    return 1.0 / (1+np.exp(-v))

"""We can use dot-product from numpy to calculate the margin and pass it to the sigmoid function"""

#w: weight vector (numpy array of size n)
#b: numpy array of size 1
#returns p(y=1|x, w, b)
def prob(x, w, b):
    return sigmoid(np.dot(x,w) + b);

"""You can also calculate $l_2$ penalty using linalg library of numpy"""

np.linalg.norm(weights)

"""$$\text{Cross Entropy Loss} = -\frac{1}{|D|}[\sum_{(y^i,\mathbf{x}^i)\in\mathcal{D}}
 y^i \log p(y=1|\mathbf{x}^i;\mathbf{w},b)  +  (1-y^i) \log (1 - p(y=1|\mathbf{x}^i;\mathbf{w},b))]+\frac{\lambda}{2} \|\mathbf{w}\|^2 $$
"""



#w: weight vector (numpy array of size n)
#x: training data points (only attributes)
#y_prob: p(y|x, w, b)
#y_true: class variable data
#lambda_: l2 penalty coefficient
#returns the cross entropy loss
def loss(w, x, y_prob, y_true, lambda_):
    m = y_true.shape[0]

    ce_loss = -np.sum(y_true * np.log(y_prob + 1e-15) + (1 - y_true) * np.log(1 - y_prob + 1e-15)) / m

    l2_reg = (lambda_ / 2) * np.sum(w ** 2)

    return ce_loss + l2_reg

#x: input variables (data of size m x n with m data point and n features)
#w: weight vector (numpy array of size n)
#y_prob: p(y|x, w, b)
#y_true: class variable data
#lambda_: l2 penalty coefficient
#returns tuple of gradient w.r.t w and w.r.t to bias

def grad_w_b(x, w, y_prob, y_true, lambda_):

    m = y_true.shape[0]

    grad_w = (np.dot(x.T, (y_prob - y_true)) / m) + (lambda_ * w)

    grad_b = np.sum(y_prob - y_true) / m

    return (grad_w, grad_b)

#lambda_ is the coeffienct of l2 norm penalty
#learning_rate is learning rate of gradient descent algorithm
#max_iter determines the maximum number of iterations if the gradients descent does not converge.
#continue the training while gradient > 0.1 or the number steps is less max_iter

#returns model as tuple of (weights,bias)

def fit(x, y_true, learning_rate, lambda_, max_iter, verbose=0):
    weights = np.random.normal(0, 1, x.shape[1]);
    bias = np.random.normal(0,1,1)
    # raise NotImplementedError
    # #change the condition appropriately
    # while True:

    #     if verbose: #verbose is used for debugging purposes
    #         #print iteration number, loss, l2 norm of gradients, l2 norm of weights
    #         pass
    # return (weights, bias)

    for i in range(max_iter):
      y_prob = prob(x, weights, bias)

      grad_w, grad_b = grad_w_b(x, weights, y_prob, y_true, lambda_)

      grad_norm = np.linalg.norm(grad_w) + abs(grad_b)

      # Update weights and bias
      weights -= learning_rate * grad_w
      bias -= learning_rate * grad_b

      if verbose:
          loss_value = loss(weights, x, y_prob, y_true, lambda_)
          weight_norm = np.linalg.norm(weights)
          print(f"Iteration {i+1}: Loss = {loss_value:.4f}, ||Grad|| = {grad_norm:.4f}, ||W|| = {weight_norm:.4f}")

      # Stop
      if grad_norm < 0.1:
          break

    return weights, bias

def accuracy(x, y_true, model):
    w, b = model
    return np.sum((prob(x, w, b)>0.5).astype(float) == y_true)  / y_true.shape[0]

learning_rate = 0.001
lambda_ = 1.0

model = fit(xtrain_normal, ytrain, learning_rate, lambda_, 10000, verbose=1) #keep the verbose on here for your submissions

print("Train accuracy: ", accuracy(xtrain_normal, ytrain, model))

#grid search for finding the best hyperparams and model

best_model = None
best_val = -1
for lr in [1,0.1,0.01, 0.001, 0.0001]:
    for la in [5,4,3, 2,1 ,0.3 ,0.2 , 0.15 , 0.05 , 0.025 , 0.02]:
        model = fit(xtrain_normal, ytrain, lr, la, 15000, verbose=0)
        val_acc = accuracy(xval_normal, yval, model)
        print(lr, la, val_acc)
        if val_acc > best_val:
            best_val = val_acc
            best_model = model

print("Test accuracy: ", accuracy(xtest_normal, ytest, best_model))